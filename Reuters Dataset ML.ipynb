{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining the Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim for this Reuters data set is to classify each of the newswires into one of 46 different classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data set has 8982 samples in the training set and 2246 samples in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are multiple classes, this is a single-label, multiclass problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choosing measure of success**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can measure how successful the model is at classifying the newswires into the different classes by checking its accuracy\n",
    "\n",
    "If the model can achieve an accuracy of at least 75%, we will consider it to be accurate and succesful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deciding an evaluation protocol**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the hold out validation method as we have sufficient data to execute this method well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load the data. We limit the dictionary to 10000 words so as to discard uncommonly used words to keep it manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import reuters\n",
    "(train_data, train_labels,), (test_data, test_labels) = reuters.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we vectorize the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorize_sequences(sequences, dimension = 10000):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, labels are one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical # one hot encoder for lists\n",
    "one_hot_train_labels = to_categorical(train_labels)\n",
    "one_hot_test_labels = to_categorical(test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Developing a model that does better than the baseline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to calculate the accuracy of the baseline model. The baseline model will predict according to the most populated class in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = np.zeros(46)\n",
    "for i in range(len(train_labels)):\n",
    "    all_labels[train_labels[i]] += 1\n",
    "index = np.argmax(all_labels)\n",
    "hits = np.array(test_labels) == index\n",
    "print(np.sum(hits)/len(test_labels) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above, the baseline model's accuracy is about 36%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial model that we will be building will be a small one that is just able to beat the baseline model's accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(16, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(16, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "history = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                    epochs = 20,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_test_results = model.evaluate(partial_x_train, partial_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above cell, this model achieves an accuracy of 96%, which is far greater than the baseline model\n",
    "\n",
    "However, this model has a low capacity, which may cause information bottleneck, so we will scale up from this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling up**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we will be scaling up our model to prevent underfitting and to achieve peak performance without overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The epoch_graphs() function will help use to plot graphs to compare the validation loss between to subsequent models we will be creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epochs_graphs(x, y_A, style_A, label_A, y_B, style_B, label_B, title, x_label, y_label):\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.clf()\n",
    "    plt.plot(x, y_A, style_A, label = label_A)\n",
    "    plt.plot(x, y_B, style_B, label = label_B)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will create a model with 32 layers and train it for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(32, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(32, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "history_32 = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                    epochs = 50,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will build a model with 64 layers and train it for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(64, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(64, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#training the model\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "history_64 = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                    epochs = 50,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, can now compare the two models by plotting the graph of their validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_loss_32 = history_32.history['val_loss']\n",
    "val_loss_64 = history_64.history['val_loss']\n",
    "\n",
    "#plotting the graph\n",
    "epochs_graphs(range(1,len(val_loss_64)+1), val_loss_64, 'bo', '64 Layer Model', val_loss_32, 'b', \n",
    "              '32 Layer Model', 'Validation loss of the 2 models', 'Epochs', 'Val Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph above shows the 64 layer has a lower validation loss than the 32 layer, hence, overfitting has not yet occured and we can still expand the model\n",
    "\n",
    "We can now build a model with 96 layers and train it for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(96, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(96, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#training the model\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "history_96 = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                    epochs = 50,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We, can now compare the two models by plotting the graph of their validation losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_loss_96 = history_96.history['val_loss']\n",
    "\n",
    "#plotting the graph\n",
    "epochs_graphs(range(1,len(val_loss_96)+1), val_loss_96, 'bo', '96 Layer Model', val_loss_64, 'b', \n",
    "              '64 Layer Model', 'Validation loss of the 2 models', 'Epochs', 'Val Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, the 96 layer model's validation loss is slightly higher than the 64 layer's after roughly 9 epochs, showing that overfitting is starting to occur.\n",
    "\n",
    "Despite that, it is able to achieve lower validation loss at a lower nuumber of epochs, thus, we can still use 96 layers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regularising the model and tuning hyperparameters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After finding a suitable model size and the right number of epochs to train it for, we can still further improve the model through weight regularisation and tuning the hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding L1 regularisation to the model:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "#building the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(96, kernel_regularizer = regularizers.L1(0.001), activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(96, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#training the model\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "history_l1 = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                    epochs = 50,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_l1 = history_l1.history['val_loss']\n",
    "\n",
    "#plotting the graph\n",
    "epochs_graphs(range(1,len(val_loss_96)+1), val_loss_96, 'bo', '96 Layer Model', val_loss_l1, 'b', \n",
    "              'L1 Model', 'Validation loss of the 2 models', 'Epochs', 'Val Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 regularization does not seem to reduce overfitting, as such will not be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding L2 regularisation to the model:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#building the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(96, kernel_regularizer = regularizers.L2(0.001), activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(96, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#training the model\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "history_l2 = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                    epochs = 50,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val_loss_l2 = history_l2.history['val_loss']\n",
    "val_loss_96 = history_96.history['val_loss']\n",
    "\n",
    "#plotting the graph\n",
    "epochs_graphs(range(1,len(val_loss_96)+1), val_loss_96, 'bo', '96 Layer Model', val_loss_l2, 'b', \n",
    "              'L2 Model', 'Validation loss of the 2 models', 'Epochs', 'Val Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularization greatly reduces overfitting over 50 epochs, but the 96 layer model without L2 regularization still achieves lower validation loss. Thus L2 regularization will not be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Adding dropout to the model:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using 50% dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#building the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(96, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(96, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#training the model\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "history_drop = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                    epochs = 50,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss_drop = history_drop.history['val_loss']\n",
    "val_loss_96 = history_96.history['val_loss']\n",
    "\n",
    "#plotting the graph\n",
    "epochs_graphs(range(1,len(val_loss_96)+1), val_loss_96, 'bo', '96 Layer Model', val_loss_drop, 'b', \n",
    "              'Dropout Model', 'Validation loss of the 2 models', 'Epochs', 'Val Loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout also does not reduce validation loss sufficiently, thus will not be used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now find the optimal number of epochs to train the model for using the graph above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmin(val_loss_96), 'epochs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this info we can finally train the final model and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the model\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(96, activation = 'relu', input_shape = (10000,)))\n",
    "model.add(layers.Dense(96, activation = 'relu'))\n",
    "model.add(layers.Dense(46, activation = 'softmax'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#training the model\n",
    "x_val = x_train[:1000]\n",
    "partial_x_train = x_train[1000:]\n",
    "y_val = one_hot_train_labels[:1000]\n",
    "partial_y_train = one_hot_train_labels[1000:]\n",
    "\n",
    "history_l2_final = model.fit(partial_x_train, \n",
    "                    partial_y_train,\n",
    "                    epochs = 5,\n",
    "                    batch_size = 512,\n",
    "                    validation_data = (x_val, y_val))\n",
    "\n",
    "results = model.evaluate(x_test, one_hot_test_labels)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is able to achieve 78% accuracy, hence we can consider it to be successful"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
